{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Matthew Twete\n",
        "\n",
        "\n",
        "Basic feedforward neural networks trained to classify the fashionMNIST dataset with various activations and hyperparameters. Models were also trained on polluted and shifted data."
      ],
      "metadata": {
        "id": "HxbFoYaJKxS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "#Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ],
      "metadata": {
        "id": "3xJKUeuGPymz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 30\n",
        "\n",
        "#Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "P3BFSmAIRkF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "#Define model for a feedforward MLP\n",
        "class NeuralNetwork(nn.Module):\n",
        "    #Updated constructor with arguments for the number layers and an optional\n",
        "    #argument if you want the activation function to be sigmoid\n",
        "    def __init__(self,numLayers,sig = False):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        if (numLayers == 2):\n",
        "          if (sig == False):\n",
        "            self.linear_relu_stack = nn.Sequential(\n",
        "                nn.Linear(28*28, 1024),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(1024, 1024),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(1024, 10)\n",
        "            )\n",
        "          else:\n",
        "            self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 1024),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(1024, 10)\n",
        "        )\n",
        "\n",
        "        else:\n",
        "          self.linear_relu_stack = nn.Sequential(\n",
        "              nn.Linear(28*28, 1024),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(1024, 10)\n",
        "          )\n",
        "    #Define function to feed data forward\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "#Create the networks \n",
        "oneLayerNN = NeuralNetwork(1).to(device)\n",
        "print(oneLayerNN)\n",
        "TwoLayerNN = NeuralNetwork(2).to(device)\n",
        "print(TwoLayerNN)"
      ],
      "metadata": {
        "id": "2z7wqoqcS3v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "IybFIWJBUX4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define training function\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "id": "-2DcEOrxUaqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define testing function\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "ALMhoERpUdAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOqqwCg1TP29"
      },
      "outputs": [],
      "source": [
        "#Train and test the two networks\n",
        "epochs = 2\n",
        "optimizer1 = torch.optim.SGD(oneLayerNN.parameters(), lr=1e-3)\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, oneLayerNN, loss_fn, optimizer1)\n",
        "    test(test_dataloader, oneLayerNN, loss_fn)\n",
        "optimizer2 = torch.optim.SGD(TwoLayerNN.parameters(), lr=1e-3)    \n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, TwoLayerNN, loss_fn, optimizer2)\n",
        "    test(test_dataloader, TwoLayerNN, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loop over all the batch sizes and learning rates for 2 epochs each \n",
        "#and test and train a 2 layer ReLU and 2 layer sigmoid model with those parameters \n",
        "batch_sizes = [1,10,100]\n",
        "learn_rate = [1,0.1,0.01,0.001]\n",
        "epochs = 2\n",
        "#Loop over batch sizes\n",
        "for size in batch_sizes:\n",
        "  #Create data loaders\n",
        "  train_dataloader = DataLoader(training_data, batch_size=size)\n",
        "  test_dataloader = DataLoader(test_data, batch_size=size)\n",
        "  #Loop over learning rates\n",
        "  for rate in learn_rate:\n",
        "    #Print the batch size and learning rate for the iteration\n",
        "    print(\"Batch size: \", size, \" Learning rate: \", rate)\n",
        "    #Set up, train and test the ReLU and sigmoid networks\n",
        "    ReLU_NN = NeuralNetwork(2).to(device)\n",
        "    Sig_NN = NeuralNetwork(2,True).to(device)\n",
        "    optimizer1 = torch.optim.SGD(ReLU_NN.parameters(), lr=rate)\n",
        "    print(\"ReLU NN\")\n",
        "    for t in range(epochs):\n",
        "      print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "      train(train_dataloader, ReLU_NN, loss_fn, optimizer1)\n",
        "      test(test_dataloader, ReLU_NN, loss_fn)\n",
        "    optimizer2 = torch.optim.SGD(Sig_NN.parameters(), lr=rate)\n",
        "    print(\"Sigmoid NN\")    \n",
        "    for t in range(epochs):\n",
        "      print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "      train(train_dataloader, Sig_NN, loss_fn, optimizer2)\n",
        "      test(test_dataloader, Sig_NN, loss_fn)\n",
        "    print(\"Done!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6rBdREqzhsOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load a copy of the training data to pollute\n",
        "polluted_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n"
      ],
      "metadata": {
        "id": "Vodkcu8H-5l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import numpy\n",
        "import numpy as np\n",
        "\n",
        "#Create an array to keep track of how many polluted images of each\n",
        "#class have been added to the other 9 classes. Each row represents a\n",
        "#class, with each column representing the remaining number of instances \n",
        "#of that row's class that need to be added to the columns class\n",
        "#Ex: row 0, column 1 represents how many instances of the 0th class need to be\n",
        "#converted to the 1st class\n",
        "pollution_counts = np.full((10,10),6)\n",
        "#Since you can't pollute a class with itself, convert the diagonals to 0\n",
        "np.fill_diagonal(pollution_counts,0)"
      ],
      "metadata": {
        "id": "EWiPH7BE6Ebw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_data = len(polluted_data.targets)\n",
        "#Loop over all the data\n",
        "for i in range(num_data):\n",
        "  #Get the label for the i'th image \n",
        "  label = polluted_data.targets[i]\n",
        "  #Check if all 9 other classes have been fully polluted by this class \n",
        "  #this will happen when the row in pollution_counts corrosponding to the class label\n",
        "  #is all 0's, so if it is not all 0s, the label needs to be changed to pollute another \n",
        "  #class \n",
        "  if (np.count_nonzero(pollution_counts[label]) != 0):\n",
        "    #Find one of the classes that needs to be polluted by the current image's class\n",
        "    new_label = np.nonzero(pollution_counts[label])[0][0]\n",
        "    #Decrement the number of remaining instances of that class that need to be polluted \n",
        "    #by the current image's class\n",
        "    pollution_counts[label][new_label] -= 1\n",
        "    #Change the label of the current image's class to the one that it needs to pollute\n",
        "    polluted_data.targets[i] = new_label\n"
      ],
      "metadata": {
        "id": "Z1ko1n-N17e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Turns out the best network from before was the ReLU model with batch size of 1 and a learning rate of 0.01, \n",
        "#so create a new network with those parameters to train with polluted data\n",
        "learn_rate = 0.01\n",
        "batch_size = 1\n",
        "polluted_model = NeuralNetwork(2).to(device)\n",
        "optimizer1 = torch.optim.SGD(polluted_model.parameters(), lr=learn_rate)\n",
        "epochs = 2\n",
        "# Create data loader for the polluted training data & normal test data\n",
        "train_dataloader = DataLoader(polluted_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "#Train and test the network\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, polluted_model, loss_fn, optimizer1)\n",
        "    test(test_dataloader, polluted_model, loss_fn)"
      ],
      "metadata": {
        "id": "9pGjgBIpyuXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I will continue to use the ReLU model with batch size of 1 and a learning rate of 0.01\n",
        "#Due to the way I structured my code, I didn't save each model, so I will retrain one\n",
        "#with the same parameters to use for part 5\n",
        "\n",
        "#Set up and train the model\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "ReLU_NN = NeuralNetwork(2).to(device)\n",
        "optimizer1 = torch.optim.SGD(ReLU_NN.parameters(), lr=learn_rate)\n",
        "for t in range(epochs):\n",
        "  train(train_dataloader, ReLU_NN, loss_fn, optimizer1)\n",
        "  test(test_dataloader, ReLU_NN, loss_fn)"
      ],
      "metadata": {
        "id": "XyKoCDu_97Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load a copy of the test data to shift\n",
        "shifted_test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "#Do a right circular shift on the data\n",
        "shifted_test_data.data = torch.roll(shifted_test_data.data, shifts=(0,0,2), dims=(0,1,2))\n",
        "#Create data loader\n",
        "shifted_test_dataloader = DataLoader(shifted_test_data, batch_size=batch_size)\n",
        "#Test the network on the shifted data\n",
        "test(shifted_test_dataloader, ReLU_NN, loss_fn)\n",
        "#Do a down circular shift and put the data in a dataloader\n",
        "shifted_test_data.data = torch.roll(shifted_test_data.data, shifts=(0,2,0), dims=(0,1,2))\n",
        "shifted_test_dataloader = DataLoader(shifted_test_data, batch_size=batch_size)\n",
        "#Test again on the network\n",
        "test(shifted_test_dataloader, ReLU_NN, loss_fn)"
      ],
      "metadata": {
        "id": "tk0do3KaM9pH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}